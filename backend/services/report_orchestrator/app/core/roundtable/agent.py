"""
Agent: The fundamental actor in the multi-agent system
Agent: å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„åŸºæœ¬è¡ŒåŠ¨è€…
"""
from typing import List, Dict, Any, Optional
from .message import Message, MessageType
from .tool import Tool
from .message_bus import MessageBus
import httpx
import json


class Agent:
    """
    AgentæŠ½è±¡åŸºç±»

    ä»£è¡¨ç³»ç»Ÿä¸­çš„è‡ªä¸»å®ä½“,è´Ÿè´£:
    - å¤„ç†æ”¶åˆ°çš„æ¶ˆæ¯
    - ä½¿ç”¨å·¥å…·è·å–ä¿¡æ¯
    - ä¸LLMäº¤äº’è¿›è¡Œæ¨ç†
    - ç”Ÿæˆå¹¶å‘é€æ–°æ¶ˆæ¯
    """

    def __init__(
        self,
        name: str,
        role_prompt: str = None,
        llm_gateway_url: str = "http://llm_gateway:8003",
        model: str = "gpt-4",
        temperature: float = 0.7,
        # Extended parameters for trading agents
        id: str = None,
        role: str = None,
        personality: str = None,
        system_prompt: str = None,
        expertise: List[str] = None,
        avatar: str = None
    ):
        """
        åˆå§‹åŒ–Agent

        Args:
            name: Agentçš„æ˜¾ç¤ºåç§°
            role_prompt: å®šä¹‰Agentçš„äººæ ¼ã€ä¸“é•¿å’Œç›®æ ‡çš„ç³»ç»Ÿæç¤º
            llm_gateway_url: LLMç½‘å…³æœåŠ¡çš„URL
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            temperature: ç”Ÿæˆæ¸©åº¦å‚æ•°
            id: Agentçš„å”¯ä¸€æ ‡è¯†ç¬¦ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨nameï¼‰
            role: Agentçš„è§’è‰²æè¿°
            personality: Agentçš„ä¸ªæ€§æè¿°
            system_prompt: ç³»ç»Ÿæç¤ºï¼ˆä¸role_promptå¯äº’æ¢ï¼‰
            expertise: Agentçš„ä¸“é•¿é¢†åŸŸåˆ—è¡¨
            avatar: Agentçš„å¤´åƒå›¾æ ‡
        """
        self.id = id or name  # Use id if provided, otherwise use name
        self.name = name
        self.role = role or name
        self.personality = personality or ""
        self.expertise = expertise or []
        self.avatar = avatar or "person"

        # System prompt can be specified as role_prompt or system_prompt
        self.role_prompt = role_prompt or system_prompt or ""
        self.system_prompt = self.role_prompt  # Alias for compatibility

        self.llm_gateway_url = llm_gateway_url
        self.model = model
        self.temperature = temperature

        # å·¥å…·æ³¨å†Œè¡¨
        self.tools: Dict[str, Tool] = {}

        # æ¶ˆæ¯å†å²ï¼ˆAgentçš„ç§æœ‰è®°å¿†ï¼‰
        self.message_history: List[Message] = []

        # MessageBuså¼•ç”¨ï¼ˆç”±Meetingè®¾ç½®ï¼‰
        self.message_bus: Optional[MessageBus] = None

        # Agentå½“å‰çŠ¶æ€
        self.status = "idle"  # idle, thinking, tool_using, speaking

    def register_tool(self, tool: Tool):
        """
        æ³¨å†Œå·¥å…·åˆ°Agentçš„å·¥å…·å¸¦

        Args:
            tool: è¦æ³¨å†Œçš„å·¥å…·
        """
        self.tools[tool.name] = tool
        print(f"[Agent:{self.name}] Tool registered: {tool.name}")

    def get_tools_schema(self) -> List[Dict[str, Any]]:
        """
        è·å–æ‰€æœ‰å·¥å…·çš„Schemaï¼ˆç”¨äºLLM function callingï¼‰

        Returns:
            å·¥å…·Schemaåˆ—è¡¨
        """
        return [tool.to_schema() for tool in self.tools.values()]

    async def think_and_act(self) -> List[Message]:
        """
        Agentçš„ä¸»å¾ªç¯ï¼šå¤„ç†æ¶ˆæ¯ã€å†³ç­–ã€ç”Ÿæˆå“åº”

        Returns:
            è¦å‘é€çš„æ¶ˆæ¯åˆ—è¡¨
        """
        if not self.message_bus:
            raise RuntimeError(f"Agent {self.name} not connected to MessageBus")

        # 1. è·å–å¾…å¤„ç†æ¶ˆæ¯
        new_messages = self.message_bus.get_messages(self.name)

        if not new_messages:
            return []

        # 2. æ›´æ–°æ¶ˆæ¯å†å²
        self.message_history.extend(new_messages)

        # 3. çŠ¶æ€æ›´æ–°ï¼šæ€è€ƒä¸­
        self.status = "thinking"

        # 4. æ„å»ºLLMæç¤º
        prompt_messages = self._build_llm_prompt()

        # 5. è°ƒç”¨LLMè¿›è¡Œæ¨ç†
        llm_response = await self._call_llm(prompt_messages)

        # 6. è§£æLLMå“åº”
        outgoing_messages = await self._parse_llm_response(llm_response)

        # 7. çŠ¶æ€æ›´æ–°ï¼šç©ºé—²
        self.status = "idle"

        return outgoing_messages

    def _build_llm_prompt(self) -> List[Dict[str, str]]:
        """
        æ„å»ºå‘é€ç»™LLMçš„æç¤ºæ¶ˆæ¯

        Returns:
            ç¬¦åˆOpenAIæ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨
        """
        messages = []

        # ç³»ç»Ÿæç¤ºï¼ˆå®šä¹‰è§’è‰²ï¼‰
        messages.append({
            "role": "system",
            "content": self._get_system_prompt()
        })

        # å¯¹è¯å†å² - ä¿ç•™æ›´å¤šä¸Šä¸‹æ–‡ä»¥é¿å…ä¸¢å¤±å…³é”®ä¿¡æ¯
        for msg in self.message_history[-20:]:  # ä¿ç•™æœ€è¿‘20æ¡
            # æ ¹æ®æ¶ˆæ¯å‘é€è€…ç¡®å®šè§’è‰²
            if msg.sender == self.name:
                role = "assistant"
            else:
                role = "user"

            content = f"[{msg.sender} â†’ {msg.recipient}] {msg.content}"
            messages.append({
                "role": role,
                "content": content
            })

        return messages

    def _get_system_prompt(self) -> str:
        """
        è·å–å®Œæ•´çš„ç³»ç»Ÿæç¤º

        Returns:
            ç³»ç»Ÿæç¤ºå­—ç¬¦ä¸²
        """
        base_prompt = f"""ä½ æ˜¯ {self.name}ï¼Œä¸€ä¸ªæŠ•èµ„åˆ†æä¸“å®¶å›¢é˜Ÿçš„æˆå‘˜ã€‚

{self.role_prompt}

## ä½ çš„å·¥ä½œæ–¹å¼:
1. **ä»”ç»†é˜…è¯»**æ‰€æœ‰æ”¶åˆ°çš„æ¶ˆæ¯å’Œè®¨è®ºå†å²
2. **åˆ†æé—®é¢˜**ä»ä½ çš„ä¸“ä¸šè§’åº¦æ€è€ƒ
3. **ä½¿ç”¨å·¥å…·**å¦‚æœéœ€è¦æ•°æ®æ”¯æŒï¼Œä½¿ç”¨ä½ çš„ä¸“ä¸šå·¥å…·
4. **è¡¨è¾¾è§‚ç‚¹**:
   - å¯ä»¥åŒæ„æˆ–åå¯¹å…¶ä»–ä¸“å®¶çš„è§‚ç‚¹
   - å¯ä»¥å‘ç‰¹å®šä¸“å®¶æé—®
   - å¯ä»¥è¯·æ±‚ä¸æŸä¸ªä¸“å®¶ç§èŠ
   - å¯ä»¥åˆ†äº«ä½ çš„æ€è€ƒè¿‡ç¨‹

## æ¶ˆæ¯æ ¼å¼è§„èŒƒ:
- **å¹¿æ’­å‘è¨€**: å‘é€ç»™ "ALL"ï¼Œæ‰€æœ‰ä¸“å®¶éƒ½èƒ½çœ‹åˆ°
- **æé—®**: ä½¿ç”¨ @ä¸“å®¶å æé—®ç‰¹å®šä¸“å®¶
- **ç§èŠ**: æ˜ç¡®è¯´æ˜ "ç§èŠç»™XXX" æˆ– "ç§ä¸‹ä¸XXXè®¨è®º"
- **è¡¨æ€**: å¯ä»¥è¯´"æˆ‘åŒæ„XXXçš„è§‚ç‚¹"æˆ–"æˆ‘ä¸åŒæ„XXXçš„çœ‹æ³•"

## ä½ å¯ç”¨çš„å·¥å…·:
"""
        # æ·»åŠ å·¥å…·æè¿°
        if self.tools:
            for tool_name, tool in self.tools.items():
                base_prompt += f"\n- **{tool_name}**: {tool.description}"
            base_prompt += """

## å¦‚ä½•ä½¿ç”¨å·¥å…·:
å½“ä½ éœ€è¦ä½¿ç”¨å·¥å…·è·å–ä¿¡æ¯æ—¶ï¼Œè¯·åœ¨å›å¤ä¸­ä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼š
[USE_TOOL: tool_name(param1="value1", param2="value2")]

ä¾‹å¦‚ï¼š
- æœç´¢ä¿¡æ¯: [USE_TOOL: tavily_search(query="ç‰¹æ–¯æ‹‰2024å¹´Q4é”€é‡")]
- æŸ¥è¯¢å…¬å¸æ•°æ®: [USE_TOOL: get_public_company_data(company_name="ç‰¹æ–¯æ‹‰")]
- æœç´¢çŸ¥è¯†åº“: [USE_TOOL: search_knowledge_base(query="ç‰¹æ–¯æ‹‰è´¢åŠ¡åˆ†æ")]

### âš ï¸ æ—¶æ•ˆæ€§æœç´¢æŒ‡å¼• (é‡è¦!):
å¯¹äºéœ€è¦æœ€æ–°ä¿¡æ¯çš„é—®é¢˜ï¼Œ**å¿…é¡»**ä½¿ç”¨æ—¶é—´è¿‡æ»¤å‚æ•°ï¼š
- æœ€è¿‘24å°æ—¶: [USE_TOOL: tavily_search(query="æ¯”ç‰¹å¸ä»·æ ¼", time_range="day")]
- æœ€è¿‘ä¸€å‘¨: [USE_TOOL: tavily_search(query="å¸‚åœºæ–°é—»", time_range="week")]
- æœ€è¿‘ä¸€ä¸ªæœˆ: [USE_TOOL: tavily_search(query="è´¢æŠ¥åˆ†æ", time_range="month")]
- æ–°é—»æœç´¢(æŒ‡å®šå¤©æ•°): [USE_TOOL: tavily_search(query="è‚¡ç¥¨åŠ¨æ€", topic="news", days=3)]

**åˆ¤æ–­æ ‡å‡†**: å¦‚æœé—®é¢˜æ¶‰åŠ"æœ€è¿‘"ã€"æœ€æ–°"ã€"ä»Šå¤©"ã€"æœ¬å‘¨"ã€"å½“å‰ä»·æ ¼"ç­‰æ—¶é—´æ•æ„Ÿè¯æ±‡ï¼Œ
æˆ–æ¶‰åŠçŸ­æœŸé¢„æµ‹ï¼ˆå¦‚"æœªæ¥3å¤©"ï¼‰ï¼Œå¿…é¡»ä½¿ç”¨time_rangeæˆ–dayså‚æ•°é™åˆ¶æœç´¢èŒƒå›´ã€‚

ä½¿ç”¨å·¥å…·åï¼Œä½ ä¼šæ”¶åˆ°å·¥å…·è¿”å›çš„ç»“æœï¼Œç„¶ååŸºäºè¿™äº›ç»“æœç»§ç»­è®¨è®ºã€‚
"""
        else:
            base_prompt += "\nï¼ˆå½“å‰æ²¡æœ‰å¯ç”¨å·¥å…·ï¼‰"

        base_prompt += """

## é‡è¦æé†’:
- ä¿æŒä¸“ä¸šä½†è‡ªç„¶çš„æ²Ÿé€šé£æ ¼
- ä»ä½ çš„ä¸“ä¸šè§’åº¦æä¾›æœ‰ä»·å€¼çš„è§è§£
- ä¸è¦é‡å¤å…¶ä»–ä¸“å®¶å·²ç»è¯´è¿‡çš„è§‚ç‚¹ï¼Œé™¤éä½ æœ‰æ–°çš„è§’åº¦
- å¦‚æœéœ€è¦æ•°æ®æ”¯æŒä½ çš„è§‚ç‚¹ï¼Œä¸»åŠ¨ä½¿ç”¨å¯ç”¨çš„å·¥å…·
- å¦‚æœä¸ç¡®å®šï¼Œå¯ä»¥å¦è¯šè¡¨è¾¾å¹¶å¯»æ±‚æ›´å¤šä¿¡æ¯
- è®°ä½è¿™æ˜¯ä¸€ä¸ªåä½œè®¨è®ºï¼Œç›®æ ‡æ˜¯å¾—å‡ºæœ€ä½³æŠ•èµ„å†³ç­–
"""
        return base_prompt

    async def _call_llm(self, messages: List[Dict[str, str]], max_retries: int = 3) -> Dict[str, Any]:
        """
        è°ƒç”¨LLMç½‘å…³è¿›è¡Œæ¨ç†ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼Œæ”¯æŒ Tool Callingï¼‰

        Args:
            messages: ç¬¦åˆOpenAIæ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

        Returns:
            LLMçš„å“åº”
        """
        import asyncio

        last_exception = None

        # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·å¯ç”¨
        has_tools = len(self.tools) > 0

        # å¦‚æœæœ‰å·¥å…·ï¼Œä½¿ç”¨æ–°çš„ Tool Calling ç«¯ç‚¹
        if has_tools:
            # ä½¿ç”¨ OpenAI å…¼å®¹çš„ /v1/chat/completions ç«¯ç‚¹
            tools_schema = self.get_tools_schema()

            request_data = {
                "messages": messages,  # ç›´æ¥ä¼ é€’ OpenAI æ ¼å¼çš„ messages
                "tools": tools_schema,
                "tool_choice": "auto",
                "temperature": self.temperature
            }

            print(f"[Agent:{self.name}] Using Tool Calling with {len(tools_schema)} tools")

            for attempt in range(max_retries):
                try:
                    async with httpx.AsyncClient(timeout=180.0) as client:
                        response = await client.post(
                            f"{self.llm_gateway_url}/v1/chat/completions",
                            json=request_data
                        )
                        response.raise_for_status()
                        result = response.json()

                        # result is already in OpenAI format
                        return result

                except httpx.ReadTimeout as e:
                    last_exception = e
                    print(f"[Agent:{self.name}] LLM timeout on attempt {attempt + 1}/{max_retries}")
                    if attempt < max_retries - 1:
                        wait_time = 2 ** (attempt + 1)
                        print(f"[Agent:{self.name}] Retrying in {wait_time} seconds...")
                        await asyncio.sleep(wait_time)
                    continue

                except Exception as e:
                    last_exception = e
                    print(f"[Agent:{self.name}] LLM call failed on attempt {attempt + 1}/{max_retries}: {e}")
                    if attempt < max_retries - 1:
                        await asyncio.sleep(2)
                    continue

            # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
            print(f"[Agent:{self.name}] All {max_retries} LLM call attempts failed")
            raise last_exception

        else:
            # æ²¡æœ‰å·¥å…·ï¼Œä½¿ç”¨æ—§çš„ /chat ç«¯ç‚¹ï¼ˆå‘åå…¼å®¹ï¼‰
            # è½¬æ¢ä¸º LLM Gateway æ ¼å¼
            # LLM Gateway æœŸæœ›: {"history": [{"role": "user", "parts": ["text"]}]}
            history = []
            for msg in messages:
                role = msg["role"]
                content = msg["content"]
                # Convert role: system/user/assistant -> user/model
                if role == "system" or role == "user":
                    history.append({"role": "user", "parts": [content]})
                elif role == "assistant":
                    history.append({"role": "model", "parts": [content]})

            request_data = {
                "history": history,
                "temperature": self.temperature  # ä¼ é€’temperatureå‚æ•°
            }

            for attempt in range(max_retries):
                try:
                    async with httpx.AsyncClient(timeout=180.0) as client:
                        response = await client.post(
                            f"{self.llm_gateway_url}/chat",
                            json=request_data
                        )
                        response.raise_for_status()
                        result = response.json()

                        # è°ƒè¯•ï¼šæ‰“å°å“åº”ç±»å‹å’Œå†…å®¹
                        print(f"[Agent:{self.name}] LLM response type: {type(result)}")

                        # è½¬æ¢å“åº”æ ¼å¼ï¼Œä½¿å…¶å…¼å®¹ OpenAI æ ¼å¼çš„è§£æ
                        # LLM Gateway è¿”å›: {"content": "text"}
                        # è½¬æ¢ä¸º: {"choices": [{"message": {"content": "text"}}]}

                        # å¤„ç†ä¸¤ç§å¯èƒ½çš„å“åº”æ ¼å¼
                        if isinstance(result, str):
                            # å¦‚æœresultæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
                            content = result
                        elif isinstance(result, dict):
                            # å¦‚æœæ˜¯å­—å…¸ï¼Œæå–contentå­—æ®µ
                            content = result.get("content", str(result))
                        else:
                            # å…¶ä»–ç±»å‹ï¼Œè½¬ä¸ºå­—ç¬¦ä¸²
                            content = str(result)

                        return {
                            "choices": [
                                {
                                    "message": {
                                        "role": "assistant",
                                        "content": content
                                    }
                                }
                            ]
                        }

                except httpx.ReadTimeout as e:
                    last_exception = e
                    print(f"[Agent:{self.name}] LLM timeout on attempt {attempt + 1}/{max_retries}")
                    if attempt < max_retries - 1:
                        # æŒ‡æ•°é€€é¿ï¼š2ç§’, 4ç§’, 8ç§’...
                        wait_time = 2 ** (attempt + 1)
                        print(f"[Agent:{self.name}] Retrying in {wait_time} seconds...")
                        await asyncio.sleep(wait_time)
                    continue

                except Exception as e:
                    last_exception = e
                    print(f"[Agent:{self.name}] LLM call failed on attempt {attempt + 1}/{max_retries}: {e}")
                    if attempt < max_retries - 1:
                        await asyncio.sleep(2)
                    continue

            # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
            print(f"[Agent:{self.name}] All {max_retries} LLM call attempts failed")
            raise last_exception

    async def _parse_llm_response(self, llm_response: Dict[str, Any]) -> List[Message]:
        """
        è§£æLLMçš„å“åº”å¹¶ç”Ÿæˆæ¶ˆæ¯ï¼ˆæ”¯æŒåŸç”Ÿ Tool Callingï¼‰

        Args:
            llm_response: LLMçš„åŸå§‹å“åº”

        Returns:
            è¦å‘é€çš„æ¶ˆæ¯åˆ—è¡¨
        """
        messages_to_send = []

        try:
            choice = llm_response["choices"][0]
            message = choice["message"]

            # æ£€æŸ¥æ˜¯å¦æœ‰åŸç”Ÿçš„ tool_calls (OpenAI æ ¼å¼)
            if message.get("tool_calls") and self.tools:
                # åŸç”Ÿ Tool Calling
                self.status = "tool_using"
                tool_results = []

                for tool_call in message["tool_calls"]:
                    tool_name = tool_call["function"]["name"]
                    tool_args_str = tool_call["function"]["arguments"]

                    if tool_name in self.tools:
                        print(f"[Agent:{self.name}] Native Tool Calling: {tool_name}")

                        try:
                            # è§£æ JSON å‚æ•°
                            import json
                            tool_args = json.loads(tool_args_str) if isinstance(tool_args_str, str) else tool_args_str

                            print(f"[Agent:{self.name}] Tool arguments: {tool_args}")

                            # æ‰§è¡Œå·¥å…·
                            tool_result = await self.tools[tool_name].execute(**tool_args)
                            print(f"[Agent:{self.name}] Tool {tool_name} result: {tool_result}")

                            # æ”¶é›†å·¥å…·ç»“æœ
                            if isinstance(tool_result, dict) and "summary" in tool_result:
                                tool_results.append(f"\n[{tool_name}ç»“æœ]: {tool_result['summary']}")
                            else:
                                tool_results.append(f"\n[{tool_name}ç»“æœ]: {str(tool_result)[:500]}")

                        except Exception as e:
                            print(f"[Agent:{self.name}] Tool execution failed: {e}")
                            tool_results.append(f"\n[{tool_name}é”™è¯¯]: {str(e)}")
                    else:
                        print(f"[Agent:{self.name}] Unknown tool: {tool_name}")

                # è¿”å›å·¥å…·ç»“æœä½œä¸ºæ¶ˆæ¯
                if tool_results:
                    combined_result = "".join(tool_results)
                    messages_to_send.append(Message(
                        agent_name=self.name,
                        content=combined_result,
                        message_type=MessageType.INFORMATION
                    ))

                self.status = "idle"
                return messages_to_send

            # æå–æ–‡æœ¬å†…å®¹
            content = message.get("content", "")

            # å‘åå…¼å®¹ï¼šæ£€æµ‹è‡ªå®šä¹‰æ ¼å¼çš„å·¥å…·è°ƒç”¨ [USE_TOOL: tool_name(params)]
            import re
            tool_pattern = r'\[USE_TOOL:\s*(\w+)\((.*?)\)\]'
            tool_matches = re.findall(tool_pattern, content)

            if tool_matches and self.tools:
                # æœ‰å·¥å…·è°ƒç”¨ï¼ˆå‘åå…¼å®¹æ¨¡å¼ï¼‰
                self.status = "tool_using"
                tool_results = []

                for tool_name, params_str in tool_matches:
                    if tool_name in self.tools:
                        print(f"[Agent:{self.name}] Legacy tool calling: {tool_name}")

                        # è§£æå‚æ•°
                        try:
                            # æ”¯æŒåŒå¼•å·å’Œå•å¼•å·: key="value" æˆ– key='value'
                            params = {}
                            # å…ˆå°è¯•åŒå¼•å·
                            param_pattern_double = r'(\w+)="([^"]*)"'
                            param_matches = re.findall(param_pattern_double, params_str)
                            # å†å°è¯•å•å¼•å·
                            if not param_matches:
                                param_pattern_single = r"(\w+)='([^']*)'"
                                param_matches = re.findall(param_pattern_single, params_str)

                            for key, value in param_matches:
                                params[key] = value

                            # æ‰§è¡Œå·¥å…·
                            tool_result = await self.tools[tool_name].execute(**params)
                            print(f"[Agent:{self.name}] Tool {tool_name} result: {tool_result}")

                            # æ”¶é›†å·¥å…·ç»“æœ
                            if isinstance(tool_result, dict) and "summary" in tool_result:
                                tool_results.append(f"\n[{tool_name}ç»“æœ]: {tool_result['summary']}")
                            else:
                                tool_results.append(f"\n[{tool_name}ç»“æœ]: {str(tool_result)[:500]}")

                        except Exception as tool_error:
                            print(f"[Agent:{self.name}] Tool {tool_name} error: {tool_error}")
                            tool_results.append(f"\n[{tool_name}é”™è¯¯]: {str(tool_error)}")

                # å¦‚æœæœ‰å·¥å…·ç»“æœï¼Œå°†å…¶æ·»åŠ åˆ°å†…å®¹ä¸­
                if tool_results:
                    content += "\n\n" + "\n".join(tool_results)

            if content:
                # åˆ†ææ¶ˆæ¯ç±»å‹å’Œç›®æ ‡æ¥æ”¶è€…
                message_type, recipient = self._analyze_message_intent(content)

                msg = Message(
                    sender=self.name,
                    recipient=recipient,
                    content=content,
                    message_type=message_type
                )

                messages_to_send.append(msg)
                self.message_history.append(msg)

        except Exception as e:
            print(f"[Agent:{self.name}] Failed to parse LLM response: {e}")
            import traceback
            traceback.print_exc()

        return messages_to_send

    def _analyze_message_intent(self, content: str) -> tuple[MessageType, str]:
        """
        åˆ†ææ¶ˆæ¯å†…å®¹ï¼Œç¡®å®šæ¶ˆæ¯ç±»å‹å’Œæ¥æ”¶è€…

        Args:
            content: æ¶ˆæ¯å†…å®¹

        Returns:
            (æ¶ˆæ¯ç±»å‹, æ¥æ”¶è€…åç§°)
        """
        content_lower = content.lower()

        # æ£€æµ‹ç§èŠæ„å›¾
        if "ç§èŠ" in content or "ç§ä¸‹" in content or "å•ç‹¬" in content:
            # å°è¯•æå–ç›®æ ‡Agentï¼ˆç®€åŒ–å¤„ç†ï¼‰
            for agent_name in self.message_bus.registered_agents:
                if agent_name.lower() in content_lower and agent_name != self.name:
                    return (MessageType.PRIVATE, agent_name)
            return (MessageType.PRIVATE, "ALL")

        # æ£€æµ‹æé—®æ„å›¾
        if "@" in content or "è¯·é—®" in content or "æƒ³é—®" in content:
            # å°è¯•æå–ç›®æ ‡Agent
            for agent_name in self.message_bus.registered_agents:
                if agent_name in content and agent_name != self.name:
                    return (MessageType.QUESTION, agent_name)
            return (MessageType.QUESTION, "ALL")

        # æ£€æµ‹èµåŒ/åå¯¹
        if "åŒæ„" in content or "èµåŒ" in content:
            return (MessageType.AGREEMENT, "ALL")
        if "ä¸åŒæ„" in content or "åå¯¹" in content:
            return (MessageType.DISAGREEMENT, "ALL")

        # é»˜è®¤ä¸ºå¹¿æ’­æ¶ˆæ¯
        return (MessageType.BROADCAST, "ALL")

    def get_conversation_context(self, limit: int = 20) -> List[Dict[str, Any]]:
        """
        è·å–å¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆç”¨äºè°ƒè¯•æˆ–å±•ç¤ºï¼‰

        Args:
            limit: è¿”å›çš„æœ€å¤§æ¶ˆæ¯æ•°

        Returns:
            æ¶ˆæ¯å†å²åˆ—è¡¨
        """
        return [msg.to_dict() for msg in self.message_history[-limit:]]

    async def analyze(self, target: Dict[str, Any], context: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        å•ç‹¬æ‰§è¡Œåˆ†æï¼ˆå…¼å®¹BaseOrchestratoræ¥å£ï¼‰

        è¿™æ˜¯ä¸€ä¸ªé€‚é…æ–¹æ³•ï¼Œå…è®¸Agentåœ¨BaseOrchestratorçš„workflowä¸­ç‹¬ç«‹æ‰§è¡Œåˆ†æï¼Œ
        è€Œä¸éœ€è¦å®Œæ•´çš„roundtable meetingç¯å¢ƒã€‚

        Args:
            target: åˆ†æç›®æ ‡æ•°æ®
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰

        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        # æ„å»ºåˆ†ææç¤º
        analysis_prompt = f"""è¯·åˆ†æä»¥ä¸‹æŠ•èµ„æ ‡çš„:

{json.dumps(target, ensure_ascii=False, indent=2)}

è¯·ä»ä½ çš„ä¸“ä¸šè§’åº¦æä¾›åˆ†æï¼ŒåŒ…æ‹¬:
1. å…³é”®å‘ç°
2. é£é™©å› ç´ 
3. ä¼˜åŠ¿åˆ†æ
4. è¯„åˆ†(1-10åˆ†)
5. æŠ•èµ„å»ºè®®

è¯·ä½¿ç”¨å·¥å…·è·å–å¿…è¦çš„æ•°æ®æ”¯æŒä½ çš„åˆ†æã€‚"""

        # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„åˆ†ææ¶ˆæ¯
        messages = [
            {
                "role": "system",
                "content": self._get_system_prompt()
            },
            {
                "role": "user",
                "content": analysis_prompt
            }
        ]

        # è°ƒç”¨LLMè¿›è¡Œåˆ†æ
        try:
            llm_response = await self._call_llm(messages)

            # è¯¦ç»†æ—¥å¿—ï¼šæ‰“å°å“åº”ç±»å‹å’Œå†…å®¹
            print(f"[Agent:{self.name}] ğŸ” DEBUG: llm_response type = {type(llm_response)}")
            print(f"[Agent:{self.name}] ğŸ” DEBUG: llm_response = {str(llm_response)[:200]}")

            # å®‰å…¨æå–content - å¤„ç†å¯èƒ½çš„ç±»å‹é—®é¢˜
            if isinstance(llm_response, str):
                # å¦‚æœå“åº”æ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
                print(f"[Agent:{self.name}] âœ… Response is string, using directly")
                content = llm_response
            elif isinstance(llm_response, dict) and "choices" in llm_response:
                # æ ‡å‡†æ ¼å¼
                print(f"[Agent:{self.name}] âœ… Response is dict with 'choices', extracting content")
                choice = llm_response["choices"][0]
                content = choice["message"].get("content", "")
            else:
                # æœªçŸ¥æ ¼å¼ï¼Œå°è¯•è½¬ä¸ºå­—ç¬¦ä¸²
                print(f"[Agent:{self.name}] âš ï¸ WARNING: Unexpected llm_response type: {type(llm_response)}")
                print(f"[Agent:{self.name}] âš ï¸ WARNING: Full response: {llm_response}")
                content = str(llm_response)

            # æ£€æµ‹å¹¶æ‰§è¡Œå·¥å…·è°ƒç”¨
            import re
            tool_pattern = r'\[USE_TOOL:\s*(\w+)\((.*?)\)\]'
            tool_matches = re.findall(tool_pattern, content)

            if tool_matches and self.tools:
                tool_results = []
                for tool_name, params_str in tool_matches:
                    if tool_name in self.tools:
                        try:
                            # è§£æå‚æ•°
                            params = {}
                            param_pattern = r'(\w+)="([^"]*)"'
                            param_matches = re.findall(param_pattern, params_str)
                            for key, value in param_matches:
                                params[key] = value

                            # æ‰§è¡Œå·¥å…·
                            tool_result = await self.tools[tool_name].execute(**params)
                            tool_results.append(f"[{tool_name}]: {tool_result}")
                        except Exception as e:
                            tool_results.append(f"[{tool_name} Error]: {str(e)}")

                # å¦‚æœæœ‰å·¥å…·ç»“æœï¼Œè¿›è¡Œç¬¬äºŒè½®åˆ†æ
                if tool_results:
                    follow_up_messages = messages + [
                        {
                            "role": "assistant",
                            "content": content
                        },
                        {
                            "role": "user",
                            "content": f"å·¥å…·è¿”å›ç»“æœ:\n{chr(10).join(tool_results)}\n\nè¯·åŸºäºè¿™äº›æ•°æ®ç»™å‡ºæœ€ç»ˆåˆ†æç»“è®ºã€‚"
                        }
                    ]
                    llm_response = await self._call_llm(follow_up_messages)
                    
                    # å®‰å…¨å¤„ç†ç¬¬äºŒè½®å“åº”
                    if isinstance(llm_response, str):
                        content = llm_response
                    elif isinstance(llm_response, dict) and "choices" in llm_response:
                        content = llm_response["choices"][0]["message"].get("content", "")
                    else:
                        content = str(llm_response)

            # è¿”å›ç»“æ„åŒ–ç»“æœ
            return {
                "agent": self.name,
                "analysis": content,
                "score": self._extract_score(content),
                "recommendation": self._extract_recommendation(content),
                "raw_output": content
            }

        except Exception as e:
            print(f"[Agent:{self.name}] analyze() failed: {e}")
            import traceback
            traceback.print_exc()
            return {
                "agent": self.name,
                "error": str(e),
                "analysis": f"åˆ†æå¤±è´¥: {str(e)}"
            }

    def _extract_score(self, content: str) -> float:
        """ä»åˆ†æå†…å®¹ä¸­æå–è¯„åˆ†"""
        import re
        # å°è¯•åŒ¹é… "è¯„åˆ†: 8/10" æˆ– "å¾—åˆ†: 8åˆ†" ç­‰æ ¼å¼
        score_patterns = [
            r'è¯„åˆ†[:ï¼š]\s*(\d+\.?\d*)',
            r'å¾—åˆ†[:ï¼š]\s*(\d+\.?\d*)',
            r'åˆ†æ•°[:ï¼š]\s*(\d+\.?\d*)',
            r'(\d+\.?\d*)/10',
            r'(\d+\.?\d*)åˆ†'
        ]
        for pattern in score_patterns:
            match = re.search(pattern, content)
            if match:
                score = float(match.group(1))
                return min(score / 10.0 if score > 10 else score, 1.0)
        return 0.5  # é»˜è®¤ä¸­ç­‰è¯„åˆ†

    def _extract_recommendation(self, content: str) -> str:
        """ä»åˆ†æå†…å®¹ä¸­æå–å»ºè®®"""
        content_lower = content.lower()
        if "å»ºè®®æŠ•èµ„" in content_lower or "æ¨èä¹°å…¥" in content_lower:
            return "BUY"
        elif "å»ºè®®è§‚å¯Ÿ" in content_lower or "ç»§ç»­å…³æ³¨" in content_lower:
            return "HOLD"
        elif "ä¸å»ºè®®" in content_lower or "å»ºè®®æ”¾å¼ƒ" in content_lower:
            return "PASS"
        else:
            return "FURTHER_DD"
