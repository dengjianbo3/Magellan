"""
Agent: The fundamental actor in the multi-agent system
Agent: å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„åŸºæœ¬è¡ŒåŠ¨è€…
"""
from typing import List, Dict, Any, Optional
from .message import Message, MessageType
from .tool import Tool
from .message_bus import MessageBus
import httpx
import json


class Agent:
    """
    AgentæŠ½è±¡åŸºç±»

    ä»£è¡¨ç³»ç»Ÿä¸­çš„è‡ªä¸»å®ä½“,è´Ÿè´£:
    - å¤„ç†æ”¶åˆ°çš„æ¶ˆæ¯
    - ä½¿ç”¨å·¥å…·è·å–ä¿¡æ¯
    - ä¸LLMäº¤äº’è¿›è¡Œæ¨ç†
    - ç”Ÿæˆå¹¶å‘é€æ–°æ¶ˆæ¯
    """

    def __init__(
        self,
        name: str,
        role_prompt: str,
        llm_gateway_url: str = "http://llm_gateway:8003",
        model: str = "gpt-4",
        temperature: float = 0.7
    ):
        """
        åˆå§‹åŒ–Agent

        Args:
            name: Agentçš„å”¯ä¸€æ ‡è¯†ç¬¦(å¦‚"MarketAnalyst")
            role_prompt: å®šä¹‰Agentçš„äººæ ¼ã€ä¸“é•¿å’Œç›®æ ‡çš„ç³»ç»Ÿæç¤º
            llm_gateway_url: LLMç½‘å…³æœåŠ¡çš„URL
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            temperature: ç”Ÿæˆæ¸©åº¦å‚æ•°
        """
        self.name = name
        self.role_prompt = role_prompt
        self.llm_gateway_url = llm_gateway_url
        self.model = model
        self.temperature = temperature

        # å·¥å…·æ³¨å†Œè¡¨
        self.tools: Dict[str, Tool] = {}

        # æ¶ˆæ¯å†å²ï¼ˆAgentçš„ç§æœ‰è®°å¿†ï¼‰
        self.message_history: List[Message] = []

        # MessageBuså¼•ç”¨ï¼ˆç”±Meetingè®¾ç½®ï¼‰
        self.message_bus: Optional[MessageBus] = None

        # Agentå½“å‰çŠ¶æ€
        self.status = "idle"  # idle, thinking, tool_using, speaking

    def register_tool(self, tool: Tool):
        """
        æ³¨å†Œå·¥å…·åˆ°Agentçš„å·¥å…·å¸¦

        Args:
            tool: è¦æ³¨å†Œçš„å·¥å…·
        """
        self.tools[tool.name] = tool
        print(f"[Agent:{self.name}] Tool registered: {tool.name}")

    def get_tools_schema(self) -> List[Dict[str, Any]]:
        """
        è·å–æ‰€æœ‰å·¥å…·çš„Schemaï¼ˆç”¨äºLLM function callingï¼‰

        Returns:
            å·¥å…·Schemaåˆ—è¡¨
        """
        return [tool.to_schema() for tool in self.tools.values()]

    async def think_and_act(self) -> List[Message]:
        """
        Agentçš„ä¸»å¾ªç¯ï¼šå¤„ç†æ¶ˆæ¯ã€å†³ç­–ã€ç”Ÿæˆå“åº”

        Returns:
            è¦å‘é€çš„æ¶ˆæ¯åˆ—è¡¨
        """
        if not self.message_bus:
            raise RuntimeError(f"Agent {self.name} not connected to MessageBus")

        # 1. è·å–å¾…å¤„ç†æ¶ˆæ¯
        new_messages = self.message_bus.get_messages(self.name)

        if not new_messages:
            return []

        # 2. æ›´æ–°æ¶ˆæ¯å†å²
        self.message_history.extend(new_messages)

        # 3. çŠ¶æ€æ›´æ–°ï¼šæ€è€ƒä¸­
        self.status = "thinking"

        # 4. æ„å»ºLLMæç¤º
        prompt_messages = self._build_llm_prompt()

        # 5. è°ƒç”¨LLMè¿›è¡Œæ¨ç†
        llm_response = await self._call_llm(prompt_messages)

        # 6. è§£æLLMå“åº”
        outgoing_messages = await self._parse_llm_response(llm_response)

        # 7. çŠ¶æ€æ›´æ–°ï¼šç©ºé—²
        self.status = "idle"

        return outgoing_messages

    def _build_llm_prompt(self) -> List[Dict[str, str]]:
        """
        æ„å»ºå‘é€ç»™LLMçš„æç¤ºæ¶ˆæ¯

        Returns:
            ç¬¦åˆOpenAIæ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨
        """
        messages = []

        # ç³»ç»Ÿæç¤ºï¼ˆå®šä¹‰è§’è‰²ï¼‰
        messages.append({
            "role": "system",
            "content": self._get_system_prompt()
        })

        # å¯¹è¯å†å²
        for msg in self.message_history[-10:]:  # åªä¿ç•™æœ€è¿‘10æ¡
            # æ ¹æ®æ¶ˆæ¯å‘é€è€…ç¡®å®šè§’è‰²
            if msg.sender == self.name:
                role = "assistant"
            else:
                role = "user"

            content = f"[{msg.sender} â†’ {msg.recipient}] {msg.content}"
            messages.append({
                "role": role,
                "content": content
            })

        return messages

    def _get_system_prompt(self) -> str:
        """
        è·å–å®Œæ•´çš„ç³»ç»Ÿæç¤º

        Returns:
            ç³»ç»Ÿæç¤ºå­—ç¬¦ä¸²
        """
        base_prompt = f"""ä½ æ˜¯ {self.name}ï¼Œä¸€ä¸ªæŠ•èµ„åˆ†æä¸“å®¶å›¢é˜Ÿçš„æˆå‘˜ã€‚

{self.role_prompt}

## ä½ çš„å·¥ä½œæ–¹å¼:
1. **ä»”ç»†é˜…è¯»**æ‰€æœ‰æ”¶åˆ°çš„æ¶ˆæ¯å’Œè®¨è®ºå†å²
2. **åˆ†æé—®é¢˜**ä»ä½ çš„ä¸“ä¸šè§’åº¦æ€è€ƒ
3. **ä½¿ç”¨å·¥å…·**å¦‚æœéœ€è¦æ•°æ®æ”¯æŒï¼Œä½¿ç”¨ä½ çš„ä¸“ä¸šå·¥å…·
4. **è¡¨è¾¾è§‚ç‚¹**:
   - å¯ä»¥åŒæ„æˆ–åå¯¹å…¶ä»–ä¸“å®¶çš„è§‚ç‚¹
   - å¯ä»¥å‘ç‰¹å®šä¸“å®¶æé—®
   - å¯ä»¥è¯·æ±‚ä¸æŸä¸ªä¸“å®¶ç§èŠ
   - å¯ä»¥åˆ†äº«ä½ çš„æ€è€ƒè¿‡ç¨‹

## æ¶ˆæ¯æ ¼å¼è§„èŒƒ:
- **å¹¿æ’­å‘è¨€**: å‘é€ç»™ "ALL"ï¼Œæ‰€æœ‰ä¸“å®¶éƒ½èƒ½çœ‹åˆ°
- **æé—®**: ä½¿ç”¨ @ä¸“å®¶å æé—®ç‰¹å®šä¸“å®¶
- **ç§èŠ**: æ˜ç¡®è¯´æ˜ "ç§èŠç»™XXX" æˆ– "ç§ä¸‹ä¸XXXè®¨è®º"
- **è¡¨æ€**: å¯ä»¥è¯´"æˆ‘åŒæ„XXXçš„è§‚ç‚¹"æˆ–"æˆ‘ä¸åŒæ„XXXçš„çœ‹æ³•"

## ä½ å¯ç”¨çš„å·¥å…·:
"""
        # æ·»åŠ å·¥å…·æè¿°
        if self.tools:
            for tool_name, tool in self.tools.items():
                base_prompt += f"\n- **{tool_name}**: {tool.description}"
            base_prompt += """

## å¦‚ä½•ä½¿ç”¨å·¥å…·:
å½“ä½ éœ€è¦ä½¿ç”¨å·¥å…·è·å–ä¿¡æ¯æ—¶ï¼Œè¯·åœ¨å›å¤ä¸­ä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼š
[USE_TOOL: tool_name(param1="value1", param2="value2")]

ä¾‹å¦‚ï¼š
- æœç´¢ä¿¡æ¯: [USE_TOOL: tavily_search(query="ç‰¹æ–¯æ‹‰2024å¹´Q4é”€é‡")]
- æŸ¥è¯¢å…¬å¸æ•°æ®: [USE_TOOL: get_public_company_data(company_name="ç‰¹æ–¯æ‹‰")]
- æœç´¢çŸ¥è¯†åº“: [USE_TOOL: search_knowledge_base(query="ç‰¹æ–¯æ‹‰è´¢åŠ¡åˆ†æ")]

ä½¿ç”¨å·¥å…·åï¼Œä½ ä¼šæ”¶åˆ°å·¥å…·è¿”å›çš„ç»“æœï¼Œç„¶ååŸºäºè¿™äº›ç»“æœç»§ç»­è®¨è®ºã€‚
"""
        else:
            base_prompt += "\nï¼ˆå½“å‰æ²¡æœ‰å¯ç”¨å·¥å…·ï¼‰"

        base_prompt += """

## é‡è¦æé†’:
- ä¿æŒä¸“ä¸šä½†è‡ªç„¶çš„æ²Ÿé€šé£æ ¼
- ä»ä½ çš„ä¸“ä¸šè§’åº¦æä¾›æœ‰ä»·å€¼çš„è§è§£
- ä¸è¦é‡å¤å…¶ä»–ä¸“å®¶å·²ç»è¯´è¿‡çš„è§‚ç‚¹ï¼Œé™¤éä½ æœ‰æ–°çš„è§’åº¦
- å¦‚æœéœ€è¦æ•°æ®æ”¯æŒä½ çš„è§‚ç‚¹ï¼Œä¸»åŠ¨ä½¿ç”¨å¯ç”¨çš„å·¥å…·
- å¦‚æœä¸ç¡®å®šï¼Œå¯ä»¥å¦è¯šè¡¨è¾¾å¹¶å¯»æ±‚æ›´å¤šä¿¡æ¯
- è®°ä½è¿™æ˜¯ä¸€ä¸ªåä½œè®¨è®ºï¼Œç›®æ ‡æ˜¯å¾—å‡ºæœ€ä½³æŠ•èµ„å†³ç­–
"""
        return base_prompt

    async def _call_llm(self, messages: List[Dict[str, str]]) -> Dict[str, Any]:
        """
        è°ƒç”¨LLMç½‘å…³è¿›è¡Œæ¨ç†

        Args:
            messages: ç¬¦åˆOpenAIæ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨

        Returns:
            LLMçš„å“åº”
        """
        async with httpx.AsyncClient(timeout=60.0) as client:
            # è½¬æ¢ä¸º LLM Gateway æ ¼å¼
            # LLM Gateway æœŸæœ›: {"history": [{"role": "user", "parts": ["text"]}]}
            history = []
            for msg in messages:
                role = msg["role"]
                content = msg["content"]
                # Convert role: system/user/assistant -> user/model
                if role == "system" or role == "user":
                    history.append({"role": "user", "parts": [content]})
                elif role == "assistant":
                    history.append({"role": "model", "parts": [content]})

            request_data = {
                "history": history
            }

            try:
                response = await client.post(
                    f"{self.llm_gateway_url}/chat",
                    json=request_data
                )
                response.raise_for_status()
                result = response.json()

                # è°ƒè¯•ï¼šæ‰“å°å“åº”ç±»å‹å’Œå†…å®¹
                print(f"[Agent:{self.name}] LLM response type: {type(result)}")

                # è½¬æ¢å“åº”æ ¼å¼ï¼Œä½¿å…¶å…¼å®¹ OpenAI æ ¼å¼çš„è§£æ
                # LLM Gateway è¿”å›: {"content": "text"}
                # è½¬æ¢ä¸º: {"choices": [{"message": {"content": "text"}}]}

                # å¤„ç†ä¸¤ç§å¯èƒ½çš„å“åº”æ ¼å¼
                if isinstance(result, str):
                    # å¦‚æœresultæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
                    content = result
                elif isinstance(result, dict):
                    # å¦‚æœæ˜¯å­—å…¸ï¼Œæå–contentå­—æ®µ
                    content = result.get("content", str(result))
                else:
                    # å…¶ä»–ç±»å‹ï¼Œè½¬ä¸ºå­—ç¬¦ä¸²
                    content = str(result)

                return {
                    "choices": [
                        {
                            "message": {
                                "role": "assistant",
                                "content": content
                            }
                        }
                    ]
                }
            except Exception as e:
                print(f"[Agent:{self.name}] LLM call failed: {e}")
                raise

    async def _parse_llm_response(self, llm_response: Dict[str, Any]) -> List[Message]:
        """
        è§£æLLMçš„å“åº”å¹¶ç”Ÿæˆæ¶ˆæ¯

        Args:
            llm_response: LLMçš„åŸå§‹å“åº”

        Returns:
            è¦å‘é€çš„æ¶ˆæ¯åˆ—è¡¨
        """
        messages_to_send = []

        try:
            choice = llm_response["choices"][0]
            message = choice["message"]

            # æå–æ–‡æœ¬å†…å®¹
            content = message.get("content", "")

            # æ£€æµ‹å·¥å…·è°ƒç”¨ - ä½¿ç”¨è‡ªå®šä¹‰æ ¼å¼ [USE_TOOL: tool_name(params)]
            import re
            tool_pattern = r'\[USE_TOOL:\s*(\w+)\((.*?)\)\]'
            tool_matches = re.findall(tool_pattern, content)

            if tool_matches and self.tools:
                # æœ‰å·¥å…·è°ƒç”¨
                self.status = "tool_using"
                tool_results = []

                for tool_name, params_str in tool_matches:
                    if tool_name in self.tools:
                        print(f"[Agent:{self.name}] Using tool: {tool_name}")

                        # è§£æå‚æ•°
                        try:
                            # æ”¯æŒåŒå¼•å·å’Œå•å¼•å·: key="value" æˆ– key='value'
                            params = {}
                            # å…ˆå°è¯•åŒå¼•å·
                            param_pattern_double = r'(\w+)="([^"]*)"'
                            param_matches = re.findall(param_pattern_double, params_str)
                            # å†å°è¯•å•å¼•å·
                            if not param_matches:
                                param_pattern_single = r"(\w+)='([^']*)'"
                                param_matches = re.findall(param_pattern_single, params_str)

                            for key, value in param_matches:
                                params[key] = value

                            # æ‰§è¡Œå·¥å…·
                            tool_result = await self.tools[tool_name].execute(**params)
                            print(f"[Agent:{self.name}] Tool {tool_name} result: {tool_result}")

                            # æ”¶é›†å·¥å…·ç»“æœ
                            if isinstance(tool_result, dict) and "summary" in tool_result:
                                tool_results.append(f"\n[{tool_name}ç»“æœ]: {tool_result['summary']}")
                            else:
                                tool_results.append(f"\n[{tool_name}ç»“æœ]: {str(tool_result)[:500]}")

                        except Exception as tool_error:
                            print(f"[Agent:{self.name}] Tool {tool_name} error: {tool_error}")
                            tool_results.append(f"\n[{tool_name}é”™è¯¯]: {str(tool_error)}")

                # å¦‚æœæœ‰å·¥å…·ç»“æœï¼Œå°†å…¶æ·»åŠ åˆ°å†…å®¹ä¸­
                if tool_results:
                    content += "\n\n" + "\n".join(tool_results)

            if content:
                # åˆ†ææ¶ˆæ¯ç±»å‹å’Œç›®æ ‡æ¥æ”¶è€…
                message_type, recipient = self._analyze_message_intent(content)

                msg = Message(
                    sender=self.name,
                    recipient=recipient,
                    content=content,
                    message_type=message_type
                )

                messages_to_send.append(msg)
                self.message_history.append(msg)

        except Exception as e:
            print(f"[Agent:{self.name}] Failed to parse LLM response: {e}")
            import traceback
            traceback.print_exc()

        return messages_to_send

    def _analyze_message_intent(self, content: str) -> tuple[MessageType, str]:
        """
        åˆ†ææ¶ˆæ¯å†…å®¹ï¼Œç¡®å®šæ¶ˆæ¯ç±»å‹å’Œæ¥æ”¶è€…

        Args:
            content: æ¶ˆæ¯å†…å®¹

        Returns:
            (æ¶ˆæ¯ç±»å‹, æ¥æ”¶è€…åç§°)
        """
        content_lower = content.lower()

        # æ£€æµ‹ç§èŠæ„å›¾
        if "ç§èŠ" in content or "ç§ä¸‹" in content or "å•ç‹¬" in content:
            # å°è¯•æå–ç›®æ ‡Agentï¼ˆç®€åŒ–å¤„ç†ï¼‰
            for agent_name in self.message_bus.registered_agents:
                if agent_name.lower() in content_lower and agent_name != self.name:
                    return (MessageType.PRIVATE, agent_name)
            return (MessageType.PRIVATE, "ALL")

        # æ£€æµ‹æé—®æ„å›¾
        if "@" in content or "è¯·é—®" in content or "æƒ³é—®" in content:
            # å°è¯•æå–ç›®æ ‡Agent
            for agent_name in self.message_bus.registered_agents:
                if agent_name in content and agent_name != self.name:
                    return (MessageType.QUESTION, agent_name)
            return (MessageType.QUESTION, "ALL")

        # æ£€æµ‹èµåŒ/åå¯¹
        if "åŒæ„" in content or "èµåŒ" in content:
            return (MessageType.AGREEMENT, "ALL")
        if "ä¸åŒæ„" in content or "åå¯¹" in content:
            return (MessageType.DISAGREEMENT, "ALL")

        # é»˜è®¤ä¸ºå¹¿æ’­æ¶ˆæ¯
        return (MessageType.BROADCAST, "ALL")

    def get_conversation_context(self, limit: int = 20) -> List[Dict[str, Any]]:
        """
        è·å–å¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆç”¨äºè°ƒè¯•æˆ–å±•ç¤ºï¼‰

        Args:
            limit: è¿”å›çš„æœ€å¤§æ¶ˆæ¯æ•°

        Returns:
            æ¶ˆæ¯å†å²åˆ—è¡¨
        """
        return [msg.to_dict() for msg in self.message_history[-limit:]]

    async def analyze(self, target: Dict[str, Any], context: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        å•ç‹¬æ‰§è¡Œåˆ†æï¼ˆå…¼å®¹BaseOrchestratoræ¥å£ï¼‰

        è¿™æ˜¯ä¸€ä¸ªé€‚é…æ–¹æ³•ï¼Œå…è®¸Agentåœ¨BaseOrchestratorçš„workflowä¸­ç‹¬ç«‹æ‰§è¡Œåˆ†æï¼Œ
        è€Œä¸éœ€è¦å®Œæ•´çš„roundtable meetingç¯å¢ƒã€‚

        Args:
            target: åˆ†æç›®æ ‡æ•°æ®
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰

        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        # æ„å»ºåˆ†ææç¤º
        analysis_prompt = f"""è¯·åˆ†æä»¥ä¸‹æŠ•èµ„æ ‡çš„:

{json.dumps(target, ensure_ascii=False, indent=2)}

è¯·ä»ä½ çš„ä¸“ä¸šè§’åº¦æä¾›åˆ†æï¼ŒåŒ…æ‹¬:
1. å…³é”®å‘ç°
2. é£é™©å› ç´ 
3. ä¼˜åŠ¿åˆ†æ
4. è¯„åˆ†(1-10åˆ†)
5. æŠ•èµ„å»ºè®®

è¯·ä½¿ç”¨å·¥å…·è·å–å¿…è¦çš„æ•°æ®æ”¯æŒä½ çš„åˆ†æã€‚"""

        # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„åˆ†ææ¶ˆæ¯
        messages = [
            {
                "role": "system",
                "content": self._get_system_prompt()
            },
            {
                "role": "user",
                "content": analysis_prompt
            }
        ]

        # è°ƒç”¨LLMè¿›è¡Œåˆ†æ
        try:
            llm_response = await self._call_llm(messages)

            # è¯¦ç»†æ—¥å¿—ï¼šæ‰“å°å“åº”ç±»å‹å’Œå†…å®¹
            print(f"[Agent:{self.name}] ğŸ” DEBUG: llm_response type = {type(llm_response)}")
            print(f"[Agent:{self.name}] ğŸ” DEBUG: llm_response = {str(llm_response)[:200]}")

            # å®‰å…¨æå–content - å¤„ç†å¯èƒ½çš„ç±»å‹é—®é¢˜
            if isinstance(llm_response, str):
                # å¦‚æœå“åº”æ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
                print(f"[Agent:{self.name}] âœ… Response is string, using directly")
                content = llm_response
            elif isinstance(llm_response, dict) and "choices" in llm_response:
                # æ ‡å‡†æ ¼å¼
                print(f"[Agent:{self.name}] âœ… Response is dict with 'choices', extracting content")
                choice = llm_response["choices"][0]
                content = choice["message"].get("content", "")
            else:
                # æœªçŸ¥æ ¼å¼ï¼Œå°è¯•è½¬ä¸ºå­—ç¬¦ä¸²
                print(f"[Agent:{self.name}] âš ï¸ WARNING: Unexpected llm_response type: {type(llm_response)}")
                print(f"[Agent:{self.name}] âš ï¸ WARNING: Full response: {llm_response}")
                content = str(llm_response)

            # æ£€æµ‹å¹¶æ‰§è¡Œå·¥å…·è°ƒç”¨
            import re
            tool_pattern = r'\[USE_TOOL:\s*(\w+)\((.*?)\)\]'
            tool_matches = re.findall(tool_pattern, content)

            if tool_matches and self.tools:
                tool_results = []
                for tool_name, params_str in tool_matches:
                    if tool_name in self.tools:
                        try:
                            # è§£æå‚æ•°
                            params = {}
                            param_pattern = r'(\w+)="([^"]*)"'
                            param_matches = re.findall(param_pattern, params_str)
                            for key, value in param_matches:
                                params[key] = value

                            # æ‰§è¡Œå·¥å…·
                            tool_result = await self.tools[tool_name].execute(**params)
                            tool_results.append(f"[{tool_name}]: {tool_result}")
                        except Exception as e:
                            tool_results.append(f"[{tool_name} Error]: {str(e)}")

                # å¦‚æœæœ‰å·¥å…·ç»“æœï¼Œè¿›è¡Œç¬¬äºŒè½®åˆ†æ
                if tool_results:
                    follow_up_messages = messages + [
                        {
                            "role": "assistant",
                            "content": content
                        },
                        {
                            "role": "user",
                            "content": f"å·¥å…·è¿”å›ç»“æœ:\n{chr(10).join(tool_results)}\n\nè¯·åŸºäºè¿™äº›æ•°æ®ç»™å‡ºæœ€ç»ˆåˆ†æç»“è®ºã€‚"
                        }
                    ]
                    llm_response = await self._call_llm(follow_up_messages)
                    content = llm_response["choices"][0]["message"].get("content", "")

            # è¿”å›ç»“æ„åŒ–ç»“æœ
            return {
                "agent": self.name,
                "analysis": content,
                "score": self._extract_score(content),
                "recommendation": self._extract_recommendation(content),
                "raw_output": content
            }

        except Exception as e:
            print(f"[Agent:{self.name}] analyze() failed: {e}")
            import traceback
            traceback.print_exc()
            return {
                "agent": self.name,
                "error": str(e),
                "analysis": f"åˆ†æå¤±è´¥: {str(e)}"
            }

    def _extract_score(self, content: str) -> float:
        """ä»åˆ†æå†…å®¹ä¸­æå–è¯„åˆ†"""
        import re
        # å°è¯•åŒ¹é… "è¯„åˆ†: 8/10" æˆ– "å¾—åˆ†: 8åˆ†" ç­‰æ ¼å¼
        score_patterns = [
            r'è¯„åˆ†[:ï¼š]\s*(\d+\.?\d*)',
            r'å¾—åˆ†[:ï¼š]\s*(\d+\.?\d*)',
            r'åˆ†æ•°[:ï¼š]\s*(\d+\.?\d*)',
            r'(\d+\.?\d*)/10',
            r'(\d+\.?\d*)åˆ†'
        ]
        for pattern in score_patterns:
            match = re.search(pattern, content)
            if match:
                score = float(match.group(1))
                return min(score / 10.0 if score > 10 else score, 1.0)
        return 0.5  # é»˜è®¤ä¸­ç­‰è¯„åˆ†

    def _extract_recommendation(self, content: str) -> str:
        """ä»åˆ†æå†…å®¹ä¸­æå–å»ºè®®"""
        content_lower = content.lower()
        if "å»ºè®®æŠ•èµ„" in content_lower or "æ¨èä¹°å…¥" in content_lower:
            return "BUY"
        elif "å»ºè®®è§‚å¯Ÿ" in content_lower or "ç»§ç»­å…³æ³¨" in content_lower:
            return "HOLD"
        elif "ä¸å»ºè®®" in content_lower or "å»ºè®®æ”¾å¼ƒ" in content_lower:
            return "PASS"
        else:
            return "FURTHER_DD"
