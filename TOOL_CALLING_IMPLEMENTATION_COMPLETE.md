# Tool Calling å®ç°å®ŒæˆæŠ¥å‘Š

## âœ… å®æ–½æ€»ç»“

å·²æˆåŠŸå®ç° LLM Gateway çš„ Tool Calling åŠŸèƒ½ï¼Œè§£å†³äº† Leader Agent æ— æ³•æ‰§è¡Œäº¤æ˜“å·¥å…·çš„é—®é¢˜ã€‚

**æ—¶é—´**: 2025-12-03
**çŠ¶æ€**: âœ… å®Œæˆå¹¶æµ‹è¯•é€šè¿‡
**å½±å“èŒƒå›´**:
- âœ… ä¸»é¡¹ç›® (Magellan)
- âœ… Trading-Standalone é¡¹ç›®

---

## ğŸ¯ é—®é¢˜æ ¹å› 

**ä¹‹å‰çš„é—®é¢˜**:
- Leader Agent åšå‡ºäº¤æ˜“å†³ç­–æ—¶è¾“å‡º `[USE_TOOL: open_short(...)]` çº¯æ–‡æœ¬
- LLM Gateway ä¸æ”¯æŒ Function Callingï¼Œå¯¼è‡´å·¥å…·æ— æ³•è¢«çœŸæ­£è°ƒç”¨
- ç»“æœï¼šæ— æ³•æ‰§è¡Œäº¤æ˜“å¼€ä»“

**æ ¹æœ¬åŸå› **:
- LLM Gateway çš„ `/chat` ç«¯ç‚¹åªæ”¯æŒçº¯æ–‡æœ¬å¯¹è¯
- Agent è™½ç„¶æœ‰ `get_tools_schema()` æ–¹æ³•ï¼Œä½† `_call_llm()` ä¸ä¼ é€’å·¥å…·å‚æ•°
- LLM è¿”å›çš„æ˜¯æ–‡æœ¬è€Œéç»“æ„åŒ–çš„ `tool_calls`

---

## ğŸ”§ å®æ–½æ–¹æ¡ˆ

### é˜¶æ®µ 1: LLM Gateway æ·»åŠ  Tool Calling æ”¯æŒ

#### ä¿®æ”¹æ–‡ä»¶
`/backend/services/llm_gateway/app/main.py`

#### æ–°å¢åŠŸèƒ½

**1. OpenAI å…¼å®¹çš„æ•°æ®æ¨¡å‹**
```python
class ChatCompletionMessage(BaseModel):
    role: Literal["system", "user", "assistant", "tool"]
    content: Optional[str] = None
    tool_calls: Optional[List[Dict[str, Any]]] = None
    tool_call_id: Optional[str] = None
    name: Optional[str] = None

class ChatCompletionRequest(BaseModel):
    messages: List[ChatCompletionMessage]
    tools: Optional[List[Dict[str, Any]]] = None
    tool_choice: Optional[Union[str, Dict[str, Any]]] = "auto"
    temperature: Optional[float] = None
    provider: Optional[Literal["gemini", "kimi", "deepseek"]] = None

class ChatCompletionResponse(BaseModel):
    choices: List[Dict[str, Any]]
    model: str
    usage: Optional[Dict[str, Any]] = None
```

**2. æ–°ç«¯ç‚¹ `/v1/chat/completions`**
```python
@app.post("/v1/chat/completions", response_model=ChatCompletionResponse)
async def chat_completions(request: ChatCompletionRequest):
    """OpenAI å…¼å®¹çš„ Chat Completions API (æ”¯æŒ Tool Calling)"""
    provider = request.provider or current_provider

    if request.tools:
        print(f"[LLM Gateway] Tool calling enabled with {len(request.tools)} tools")

    if provider == "gemini":
        return await call_gemini_with_tools(request)
    elif provider == "deepseek":
        return await call_deepseek_with_tools(request)
    elif provider == "kimi":
        return await call_kimi_with_tools(request)
```

**3. ä¸‰ä¸ª LLM æä¾›å•†çš„ Tool Calling å®ç°**

##### Gemini Tool Calling
```python
async def call_gemini_with_tools(request: ChatCompletionRequest):
    # è½¬æ¢ OpenAI tools â†’ Gemini FunctionDeclaration
    function_declarations = convert_openai_to_gemini_tools(request.tools)

    # è½¬æ¢ OpenAI messages â†’ Gemini Content
    contents = convert_openai_to_gemini_messages(request.messages)

    # è°ƒç”¨ Gemini with function_declarations
    config = types.GenerateContentConfig(
        tools=[types.Tool(function_declarations=function_declarations)],
        temperature=request.temperature
    )

    response = gemini_client.models.generate_content(
        model=settings.GEMINI_MODEL_NAME,
        contents=contents,
        config=config
    )

    # è½¬æ¢ Gemini function_call â†’ OpenAI tool_calls
    return convert_gemini_to_openai_response(response, settings.GEMINI_MODEL_NAME)
```

##### DeepSeek Tool Calling
```python
async def call_deepseek_with_tools(request: ChatCompletionRequest):
    # DeepSeek ä½¿ç”¨ OpenAI SDKï¼ŒåŸç”Ÿæ”¯æŒ
    messages = [msg.dict(exclude_none=True) for msg in request.messages]

    response = await loop.run_in_executor(
        None,
        lambda: deepseek_client.chat.completions.create(
            model=settings.DEEPSEEK_MODEL_NAME,
            messages=messages,
            tools=request.tools,
            tool_choice=request.tool_choice,
            temperature=request.temperature
        )
    )

    return response.model_dump()  # å·²æ˜¯ OpenAI æ ¼å¼
```

##### Kimi Tool Calling
```python
async def call_kimi_with_tools(request: ChatCompletionRequest):
    # Kimi ä½¿ç”¨ OpenAI SDKï¼ŒåŸç”Ÿæ”¯æŒ
    messages = [msg.dict(exclude_none=True) for msg in request.messages]

    response = await loop.run_in_executor(
        None,
        lambda: kimi_client.chat.completions.create(
            model=settings.KIMI_MODEL_NAME,
            messages=messages,
            tools=request.tools,
            tool_choice=request.tool_choice,
            temperature=request.temperature or 0.6
        )
    )

    return response.model_dump()  # å·²æ˜¯ OpenAI æ ¼å¼
```

**4. æ ¼å¼è½¬æ¢å‡½æ•°**

```python
def convert_openai_to_gemini_tools(openai_tools):
    """å°† OpenAI tools æ ¼å¼è½¬ä¸º Gemini FunctionDeclaration"""
    function_declarations = []
    for tool in openai_tools:
        func = tool.get("function", {})
        function_declarations.append(
            types.FunctionDeclaration(
                name=func.get("name"),
                description=func.get("description", ""),
                parameters=func.get("parameters", {})
            )
        )
    return function_declarations

def convert_gemini_to_openai_response(gemini_response, model_name):
    """å°† Gemini function_call è½¬ä¸º OpenAI tool_calls æ ¼å¼"""
    candidate = gemini_response.candidates[0]
    content_parts = candidate.content.parts

    message = {"role": "assistant"}
    tool_calls = []
    text_parts = []

    for part in content_parts:
        if hasattr(part, 'function_call') and part.function_call:
            fc = part.function_call
            tool_calls.append({
                "id": f"call_{len(tool_calls)}",
                "type": "function",
                "function": {
                    "name": fc.name,
                    "arguments": json.dumps(dict(fc.args))
                }
            })
        elif hasattr(part, 'text') and part.text:
            text_parts.append(part.text)

    if tool_calls:
        message["tool_calls"] = tool_calls
        message["content"] = None
    else:
        message["content"] = "".join(text_parts)

    return {
        "choices": [{
            "message": message,
            "finish_reason": "tool_calls" if tool_calls else "stop"
        }],
        "model": model_name
    }
```

### é˜¶æ®µ 2: Agent æ”¯æŒ Tool Calling

#### ä¿®æ”¹æ–‡ä»¶
`/backend/services/report_orchestrator/app/core/roundtable/agent.py`

#### æ ¸å¿ƒä¿®æ”¹

**1. `_call_llm()` æ–¹æ³• (ç¬¬239-381è¡Œ)**

```python
async def _call_llm(self, messages: List[Dict[str, str]], max_retries: int = 3):
    """è°ƒç”¨LLMç½‘å…³è¿›è¡Œæ¨ç†ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼Œæ”¯æŒ Tool Callingï¼‰"""
    has_tools = len(self.tools) > 0

    if has_tools:
        # ä½¿ç”¨ OpenAI å…¼å®¹çš„ /v1/chat/completions ç«¯ç‚¹
        tools_schema = self.get_tools_schema()

        request_data = {
            "messages": messages,  # OpenAI æ ¼å¼
            "tools": tools_schema,
            "tool_choice": "auto",
            "temperature": self.temperature
        }

        print(f"[Agent:{self.name}] Using Tool Calling with {len(tools_schema)} tools")

        response = await client.post(
            f"{self.llm_gateway_url}/v1/chat/completions",
            json=request_data
        )
        result = response.json()
        return result  # å·²æ˜¯ OpenAI æ ¼å¼

    else:
        # å‘åå…¼å®¹ï¼šä½¿ç”¨æ—§çš„ /chat ç«¯ç‚¹
        # ... åŸæœ‰é€»è¾‘
```

**2. `_parse_llm_response()` æ–¹æ³• (ç¬¬383-550è¡Œ)**

```python
async def _parse_llm_response(self, llm_response: Dict[str, Any]):
    """è§£æLLMçš„å“åº”å¹¶ç”Ÿæˆæ¶ˆæ¯ï¼ˆæ”¯æŒåŸç”Ÿ Tool Callingï¼‰"""
    choice = llm_response["choices"][0]
    message = choice["message"]

    # ä¼˜å…ˆæ£€æŸ¥åŸç”Ÿ tool_calls (OpenAI æ ¼å¼)
    if message.get("tool_calls") and self.tools:
        self.status = "tool_using"
        tool_results = []

        for tool_call in message["tool_calls"]:
            tool_name = tool_call["function"]["name"]
            tool_args_str = tool_call["function"]["arguments"]

            if tool_name in self.tools:
                print(f"[Agent:{self.name}] Native Tool Calling: {tool_name}")

                # è§£æ JSON å‚æ•°
                tool_args = json.loads(tool_args_str) if isinstance(tool_args_str, str) else tool_args_str
                print(f"[Agent:{self.name}] Tool arguments: {tool_args}")

                # æ‰§è¡Œå·¥å…·
                tool_result = await self.tools[tool_name].execute(**tool_args)
                print(f"[Agent:{self.name}] Tool {tool_name} result: {tool_result}")

                # æ”¶é›†ç»“æœ
                if isinstance(tool_result, dict) and "summary" in tool_result:
                    tool_results.append(f"\n[{tool_name}ç»“æœ]: {tool_result['summary']}")
                else:
                    tool_results.append(f"\n[{tool_name}ç»“æœ]: {str(tool_result)[:500]}")

        if tool_results:
            combined_result = "".join(tool_results)
            messages_to_send.append(Message(
                agent_name=self.name,
                content=combined_result,
                message_type=MessageType.INFORMATION
            ))

        self.status = "idle"
        return messages_to_send

    # å‘åå…¼å®¹ï¼šæ£€æµ‹è‡ªå®šä¹‰æ ¼å¼ [USE_TOOL: ...]
    # ... åŸæœ‰é€»è¾‘
```

---

## âœ… éªŒè¯ç»“æœ

### ä¸»é¡¹ç›®æµ‹è¯•

**æµ‹è¯•å‘½ä»¤**:
```bash
curl -X POST http://localhost:8000/api/trading/trigger
```

**æµ‹è¯•ç»“æœ**:
```
Found executed decision tool: open_long with params: {
    'leverage': '15',
    'amount_usdt': '3000',
    'take_profit_percentage': '6.0',
    'stop_loss_percentage': '2.8',
    'reason': "å…¨å‘˜å¼ºå…±è¯†çœ‹å¤šï¼šSECæ’¤è¯‰é‡å¤§åˆ©å¥½å åŠ æŠ€æœ¯çªç ´($92k)ï¼Œä¸”å¸‚åœºå¤„äº'ææ…Œä¸­ä¸Šæ¶¨'çš„å¥åº·çŠ¶æ€ï¼Œé‡åŒ–æ˜¾ç¤ºç°è´§é©±åŠ¨ã€‚ä¿¡å¿ƒåº¦90%ï¼Œé‡‡ç”¨15å€æ æ†é¡¶æ ¼ä»“ä½åšå¼ˆåŠ é€Ÿä¸Šæ¶¨ã€‚"
}
```

âœ… **æˆåŠŸ**ï¼šLeader Agent é€šè¿‡ Tool Calling æˆåŠŸè°ƒç”¨ `open_long` å·¥å…·

### æ”¯æŒçš„ LLM æä¾›å•†

| æä¾›å•† | çŠ¶æ€ | å®ç°æ–¹å¼ | æµ‹è¯• |
|-------|------|---------|------|
| **Gemini** | âœ… æ”¯æŒ | Gemini native Function Calling API | âœ… å·²æµ‹è¯• |
| **DeepSeek** | âœ… æ”¯æŒ | OpenAI SDK (åŸç”Ÿæ”¯æŒ) | âœ… å·²æµ‹è¯• |
| **Kimi** | âœ… æ”¯æŒ | OpenAI SDK (åŸç”Ÿæ”¯æŒ) | âœ… å·²æµ‹è¯• |

### Trading-Standalone åŒæ­¥çŠ¶æ€

**çŠ¶æ€**: âœ… **è‡ªåŠ¨åŒæ­¥**

Trading-Standalone é¡¹ç›®é€šè¿‡ docker-compose.yml çš„ `context` é…ç½®ä½¿ç”¨ä¸»é¡¹ç›®çš„ä»£ç ï¼š

```yaml
# trading-standalone/docker-compose.yml
llm_gateway:
  build:
    context: ../backend/services/llm_gateway  # ä½¿ç”¨ä¸»é¡¹ç›®ä»£ç 

trading_service:
  build:
    context: ../backend/services/report_orchestrator  # ä½¿ç”¨ä¸»é¡¹ç›®ä»£ç 
```

**ç»“è®º**: æ— éœ€å•ç‹¬ä¿®æ”¹ï¼Œtrading-standalone è‡ªåŠ¨è·å¾— Tool Calling åŠŸèƒ½

---

## ğŸ”„ å·¥ä½œæµç¨‹å¯¹æ¯”

### ä¿®å¤å‰
```
Agent â†’ LLM Gateway /chat (çº¯æ–‡æœ¬) â†’ Gemini/DeepSeek â†’ çº¯æ–‡æœ¬å“åº”
Leader è¾“å‡º: "[USE_TOOL: open_short(...)]" (ä»…æ–‡æœ¬ï¼Œéœ€æ‰‹åŠ¨è§£æ)
ç»“æœ: æ— æ³•æ‰§è¡Œå·¥å…·ï¼Œæ— äº¤æ˜“è®°å½•
```

### ä¿®å¤å
```
Agent (with tools) â†’ LLM Gateway /v1/chat/completions â†’
  â†’ LLM (Function Calling) â†’
  â†’ ç»“æ„åŒ– tool_calls å“åº” â†’
  â†’ Agent è§£æå¹¶æ‰§è¡Œå·¥å…· â†’
  â†’ æˆåŠŸå¼€ä»“ï¼Œè®°å½•åˆ° Paper Trader
```

---

## ğŸ“‹ å…³é”®æ—¥å¿—

### LLM Gateway æ—¥å¿—
```
[LLM Gateway] Chat completions request using provider: deepseek
[LLM Gateway] Tool calling enabled with 3 tools
[DeepSeek Tool Calling] Configured 3 tools
```

### Agent æ—¥å¿—
```
[Agent:Leader] Using Tool Calling with 3 tools
[Agent:Leader] Native Tool Calling: open_long
[Agent:Leader] Tool arguments: {'leverage': '15', 'amount_usdt': '3000', ...}
[Agent:Leader] Tool open_long result: {...}
```

### Trading æ—¥å¿—
```
Found executed decision tool: open_long with params: {...}
[BROADCAST] type=agent_message, clients=1, agent=Leader
```

---

## ğŸ¯ å‘åå…¼å®¹æ€§

### ä¿ç•™åŠŸèƒ½
- âœ… æ—§çš„ `/chat` ç«¯ç‚¹ä»ç„¶å¯ç”¨
- âœ… æ— å·¥å…·çš„ Agent è‡ªåŠ¨ä½¿ç”¨æ—§ç«¯ç‚¹
- âœ… è‡ªå®šä¹‰æ ¼å¼ `[USE_TOOL: ...]` ä»ç„¶æ”¯æŒï¼ˆé™çº§æ¨¡å¼ï¼‰

### è‡ªåŠ¨åˆ‡æ¢é€»è¾‘
```python
if has_tools:
    # ä½¿ç”¨æ–°çš„ Tool Calling ç«¯ç‚¹
    endpoint = "/v1/chat/completions"
else:
    # ä½¿ç”¨æ—§çš„æ–‡æœ¬ç«¯ç‚¹
    endpoint = "/chat"
```

---

## ğŸ“Š æŠ€æœ¯ç»†èŠ‚

### OpenAI å·¥å…·æ ¼å¼
```json
{
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "open_short",
        "description": "å¼€ç©ºä»“ï¼ˆåšç©ºï¼‰",
        "parameters": {
          "type": "object",
          "properties": {
            "leverage": {"type": "integer"},
            "amount_usdt": {"type": "number"},
            "tp_percent": {"type": "number"},
            "sl_percent": {"type": "number"}
          },
          "required": ["leverage", "amount_usdt", "tp_percent", "sl_percent"]
        }
      }
    }
  ]
}
```

### Gemini å·¥å…·æ ¼å¼
```python
FunctionDeclaration(
    name="open_short",
    description="å¼€ç©ºä»“ï¼ˆåšç©ºï¼‰",
    parameters={
        "type": "object",
        "properties": {
            "leverage": {"type": "integer"},
            "amount_usdt": {"type": "number"},
            "tp_percent": {"type": "number"},
            "sl_percent": {"type": "number"}
        },
        "required": ["leverage", "amount_usdt", "tp_percent", "sl_percent"]
    }
)
```

### tool_calls å“åº”æ ¼å¼
```json
{
  "choices": [
    {
      "message": {
        "role": "assistant",
        "content": null,
        "tool_calls": [
          {
            "id": "call_0",
            "type": "function",
            "function": {
              "name": "open_short",
              "arguments": "{\"leverage\": 15, \"amount_usdt\": 2000, ...}"
            }
          }
        ]
      },
      "finish_reason": "tool_calls"
    }
  ],
  "model": "gemini-2.5-flash"
}
```

---

## âœ… éªŒè¯æ¸…å•

- [x] LLM Gateway æ·»åŠ  `/v1/chat/completions` ç«¯ç‚¹
- [x] å®ç° Gemini Tool Calling (FunctionDeclaration è½¬æ¢)
- [x] å®ç° DeepSeek Tool Calling (OpenAI SDK)
- [x] å®ç° Kimi Tool Calling (OpenAI SDK)
- [x] Agent `_call_llm()` ä¼ é€’ tools å‚æ•°
- [x] Agent `_parse_llm_response()` å¤„ç† tool_calls
- [x] ä¸»é¡¹ç›®æµ‹è¯•é€šè¿‡ï¼ˆLeader æˆåŠŸæ‰§è¡Œ open_longï¼‰
- [x] Trading-Standalone è‡ªåŠ¨åŒæ­¥
- [x] å‘åå…¼å®¹æ€§éªŒè¯ï¼ˆæ— å·¥å…·çš„ Agent æ­£å¸¸å·¥ä½œï¼‰
- [x] ä¸‰ä¸ª LLM æä¾›å•†å…¨éƒ¨æ”¯æŒ

---

## ğŸš€ åç»­å»ºè®®

### å¯é€‰ä¼˜åŒ–

1. **æ·»åŠ å·¥å…·æ‰§è¡Œæ—¥å¿—åˆ°æ•°æ®åº“**
   - è®°å½•æ¯æ¬¡å·¥å…·è°ƒç”¨çš„å‚æ•°å’Œç»“æœ
   - ä¾¿äºå®¡è®¡å’Œè°ƒè¯•

2. **æ”¯æŒå¼‚æ­¥å·¥å…·æ‰§è¡Œ**
   - å¯¹äºé•¿æ—¶é—´è¿è¡Œçš„å·¥å…·ï¼Œæ”¯æŒå¼‚æ­¥æ‰§è¡Œ
   - é¿å…é˜»å¡ Agent å“åº”

3. **å·¥å…·è°ƒç”¨é‡è¯•æœºåˆ¶**
   - å¯¹å¤±è´¥çš„å·¥å…·è°ƒç”¨è‡ªåŠ¨é‡è¯•
   - è®¾ç½®æœ€å¤§é‡è¯•æ¬¡æ•°å’Œå»¶è¿Ÿ

4. **å·¥å…·æƒé™æ§åˆ¶**
   - ä¸åŒ Agent å¯ä»¥è®¿é—®ä¸åŒçš„å·¥å…·é›†
   - é¿å…è¯¯ç”¨é«˜é£é™©å·¥å…·

### ç›‘æ§æŒ‡æ ‡

å»ºè®®ç›‘æ§ä»¥ä¸‹æŒ‡æ ‡ï¼š
- Tool Calling æˆåŠŸç‡
- å·¥å…·æ‰§è¡Œæ—¶é—´
- å·¥å…·è°ƒç”¨é¢‘ç‡
- LLM æä¾›å•†å“åº”æ—¶é—´

---

## ğŸ“ æ€»ç»“

âœ… **æˆåŠŸå®ç°** LLM Gateway çš„åŸç”Ÿ Tool Calling æ”¯æŒ

âœ… **è§£å†³é—®é¢˜**: Leader Agent ç°åœ¨èƒ½å¤ŸæˆåŠŸæ‰§è¡Œäº¤æ˜“å·¥å…·

âœ… **å…¨é¢å…¼å®¹**: æ”¯æŒ Geminiã€DeepSeekã€Kimi ä¸‰ä¸ª LLM æä¾›å•†

âœ… **è‡ªåŠ¨åŒæ­¥**: Trading-Standalone æ— éœ€é¢å¤–é…ç½®å³å¯ä½¿ç”¨

âœ… **å‘åå…¼å®¹**: ä¸å½±å“ç°æœ‰æ— å·¥å…·çš„ Agent

**ä¿®å¤æ•ˆæœ**: ä»"æ— æ³•æ‰§è¡Œäº¤æ˜“"åˆ°"æˆåŠŸè°ƒç”¨å·¥å…·å¹¶å¼€ä»“" ğŸ‰

---

**å®æ–½æ—¥æœŸ**: 2025-12-03
**å®æ–½äººå‘˜**: Claude Code
**ç‰ˆæœ¬**: v1.0.0
